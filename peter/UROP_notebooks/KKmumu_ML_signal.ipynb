{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2i3JZfAF1PUe"
   },
   "source": [
    "# $B_s \\rightarrow KK\\mu\\mu$ Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LUtgyky11P-z"
   },
   "source": [
    "This investigation has been detailed in 2 parts:\n",
    "1. Background Reduction and Decay Reconstruction\n",
    "2. Machine Learning and Branching Fraction (this notebook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gc2cVMfU1QNk"
   },
   "source": [
    "# Colab Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vHP706d32dUH"
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGu3VfN62e3z"
   },
   "source": [
    "While the Colab Notebook comes with `xgboost` installed, we have found it more suitable to upgrade it to the latest 1.X version instead of the preinstalled 0.9.0 due to increased stability in terms of model IO (i.e. saving to JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20567,
     "status": "ok",
     "timestamp": 1584617998820,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "iyit7exw2TjD",
    "outputId": "62de48ce-bdef-4363-9adf-c08bbfd78cc6"
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lxcpyai1S49"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWucMJIt1alJ"
   },
   "source": [
    "Data can be loaded from the saved output from the previous notebook (Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import matplotlib.pyplot as plt\n",
    "\"\"\"\n",
    "=============\n",
    "plot settings\n",
    "=============\n",
    "\"\"\"\n",
    "\n",
    "override = {\n",
    "   'axes.labelsize':40,\n",
    "   'font.size': 40,\n",
    "   'legend.fontsize': 40,\n",
    "   'legend.framealpha': 1,\n",
    "   'legend.edgecolor': 'black',\n",
    "   'grid.color': 'gainsboro',\n",
    "   'grid.linewidth': 1.75,\n",
    "   'xtick.labelsize': 40,\n",
    "   'ytick.labelsize': 40,\n",
    "   'figure.figsize': [13, 10],\n",
    "   'lines.linewidth': 3,\n",
    "   'font.family': 'DejaVu Sans',\n",
    "   'mathtext.fontset': 'cm',\n",
    "   'text.usetex': True\n",
    "   } \n",
    "\n",
    "overridesmall = {\n",
    "   'axes.labelsize':28,\n",
    "   'font.size': 28,\n",
    "   'legend.fontsize': 32,\n",
    "   'legend.framealpha': 1,\n",
    "   'legend.edgecolor': 'black',\n",
    "   'grid.color': 'gainsboro',\n",
    "   'xtick.labelsize': 28,\n",
    "   'ytick.labelsize': 28,   \n",
    "   'figure.figsize': [11, 8.5],\n",
    "   'lines.linewidth': 3,\n",
    "   'font.sans-serif': 'Computer Modern Sans serif',\n",
    "   'mathtext.fontset': 'cm'\n",
    "   } \n",
    "\n",
    "colors=[\"indianred\",\"dodgerblue\",\"darkorange\",\"forestgreen\",\"hotpink\",\n",
    "        \"cornflowerblue\",\"darkgoldenrod\",\"sienna\",\"yellowgreen\",\"tomato\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set xtick.major.size to 1.5 from 3.5\n",
      "Set xtick.major.pad to 1.5 from 3.5\n",
      "Set axes.labelpad to 2 from 4.0\n",
      "Set axes.grid to True from False\n"
     ]
    }
   ],
   "source": [
    "plt.rcdefaults()\n",
    "\n",
    "rcParams = {\n",
    "    'xtick.major.size': 1.5,\n",
    "    'xtick.major.pad': 1.5,\n",
    "    'axes.labelpad': 2,\n",
    "    'axes.grid': True,\n",
    "}\n",
    "for k, v in rcParams.items():\n",
    "    print(f\"Set {k} to {v} from {plt.rcParams[k]}\")\n",
    "    plt.rcParams[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_counter(data_frame):\n",
    "    actual_events=[]\n",
    "    counter=0\n",
    "    event_number=data_frame['eventNumber'].to_numpy()\n",
    "    run_number=data_frame['eventNumber'].to_numpy()\n",
    "    for i in range(len(event_number)):\n",
    "        if (event_number[i] in actual_events)==False:\n",
    "            actual_events.append(event_number[i])\n",
    "            counter+=1\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and check it's what we want. First the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJuY12g41jYt"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Bs_reduced_tau_reconstructed_data_frame_SL.pkl.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-275daa80b3f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mBs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Bs_reduced_tau_reconstructed_data_frame_SL.pkl.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mphi3mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBs_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phi_M'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBs_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phi_M'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m1820\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Bs_reduced_tau_reconstructed_data_frame_SL.pkl.gz'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Bs_df = pd.read_pickle('Bs_reduced_tau_reconstructed_data_frame_SL.pkl.gz')\n",
    "\n",
    "phi3mask=((Bs_df['phi_M']>1600) & (Bs_df['phi_M']<1820))\n",
    "\n",
    "Bs_df=Bs_df.loc[phi3mask] # apply phi3 selection\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df['phi_M'], bins=100)\n",
    "plt.xlabel(\"$m_{KK}$\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df['Bs_M'], bins=100)\n",
    "plt.xlabel(\"$m_{KK \\mu \\mu}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the MC (both for signal and for control channel phimumu that we'll use as sanity check):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bs_df_signal = pd.read_pickle('Bs_reduced_tau_reconstructed_data_frame_MC.pkl.gz')\n",
    "Bs_df_phimumu = pd.read_pickle('Bs_reduced_tau_reconstructed_data_frame_MC_phimumu.pkl.gz')\n",
    "\n",
    "truthmatch_phimumuMC= (\n",
    "    (Bs_df_phimumu['K_plus_TRUEID']==321) & (Bs_df_phimumu['K_minus_TRUEID']==-321) &\n",
    "    ((Bs_df_phimumu['mu_minus_MC_MOTHER_ID']==531)|(Bs_df_phimumu['mu_minus_MC_MOTHER_ID']==-531))\n",
    ")\n",
    "\n",
    "truthmatch_signalMC= (\n",
    "    (Bs_df_signal['K_plus_TRUEID']==321) & (Bs_df_signal['K_minus_TRUEID']==-321) & \n",
    "    (Bs_df_signal['phi_TRUEID']==337) & (Bs_df_signal['mu_minus_MC_MOTHER_ID']==15) & (Bs_df_signal['mu_plus_MC_MOTHER_ID']==-15) & ((Bs_df_signal['mu_minus_MC_GD_MOTHER_ID']==531)|(Bs_df_signal['mu_minus_MC_GD_MOTHER_ID']==-531))\n",
    ")\n",
    "\n",
    "Bs_df_phimumu=Bs_df_phimumu.loc[truthmatch_phimumuMC]\n",
    "Bs_df_signal=Bs_df_signal.loc[truthmatch_signalMC]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df_signal['phi_M'])\n",
    "plt.xlabel(\"$m_{KK}$\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df_signal['Bs_M'])\n",
    "plt.xlabel(\"$m_{KK \\mu \\mu}$\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df_phimumu['phi_M'], bins=100)\n",
    "plt.xlabel(\"$m_{KK}$\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(Bs_df_phimumu['Bs_M'], bins=100)\n",
    "plt.xlabel(\"$m_{KK \\mu \\mu}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GNwgu3y92crr"
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbnR6xYu37gO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "masses = {'mu': 105.658, 'tau': 1777, 'proton': 938.272, 'K': 493.677, 'pi': 139.57, 'D0': 1865,\n",
    "          'J/psi': 3097, 'psi(2S)': 3686, 'rho0': 770, 'rho1450': 1450, 'kstar': 892,\n",
    "          'Lc': 2286, 'Lb': 5620, 'B': 5279}\n",
    "\n",
    "\n",
    "def get_mass(data_frame: pd.DataFrame, particles_associations: List[List[str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the mass of different associations of particles\n",
    "    :param data_frame:\n",
    "    :param particles_associations: list of lists made of ['particle_P', 'particle']\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    energy_series = [\n",
    "        (data_frame.loc[:, particle_P] ** 2 + masses[particle] ** 2) ** 0.5\n",
    "        for particle_P, particle in particles_associations\n",
    "    ]\n",
    "    energy = pd.concat(energy_series, axis=1).sum(axis=1)\n",
    "    \n",
    "    particle_Ps = [i[0] for i in particles_associations]\n",
    "    \n",
    "    momentums = {}\n",
    "    for coord in ['X', 'Y', 'Z']:\n",
    "        particle_P_coords = [f'{particle_P}{coord}' for particle_P in particle_Ps]\n",
    "        momentum_series = data_frame.loc[:, particle_P_coords].sum(axis=1)\n",
    "        momentums[coord] = momentum_series\n",
    "    sum_momenta = (pd.concat(momentums.values(), axis=1) ** 2).sum(axis=1)\n",
    "    mass = (energy ** 2 - sum_momenta) ** 0.5\n",
    "    return mass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQpec--L4lHS"
   },
   "outputs": [],
   "source": [
    "particles_associations = [['K_minus_P', 'K'], ['K_plus_P', 'K'], ['mu_minus_P', 'mu'], ['mu_plus_P', 'mu']]\n",
    "Bs_df['_kkmumu_mass'] = get_mass(data_frame=Bs_df, particles_associations=particles_associations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BX23NDr-L06A"
   },
   "outputs": [],
   "source": [
    "#BDT_COLUMNS = ['Bs_ENDVERTEX_CHI2', 'Bs_FDCHI2_OWNPV',\n",
    "#    'mu_plus_IPCHI2_OWNPV', 'mu_minus_IPCHI2_OWNPV', 'K_plus_IPCHI2_OWNPV', 'K_minus_IPCHI2_OWNPV','_tau_minus_closest_dist',\n",
    "#    '_tau_plus_closest_dist','_tau_minus_flight_dist','_tau_plus_flight_dist','_bs_angle_z','_taus_angle']\n",
    "\n",
    "\n",
    "BDT_COLUMNS = ['mu_plus_PT', 'mu_minus_PT', 'mu_plus_P', 'mu_minus_P','_tau_minus_closest_dist',\n",
    "    '_tau_plus_closest_dist','_tau_minus_flight_dist','_tau_plus_flight_dist','K_minus_PIDK','K_plus_PIDK','phi_ORIVX_CHI2','_tau_minus_phi_IP','_tau_plus_phi_IP',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tye3q_e73Bm6"
   },
   "source": [
    "# Signal region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zmtLEj-KCg71"
   },
   "source": [
    "## Finding the signal region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PTfpNdCI3GCd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "percentiles = [0.00135, 0.02275, 0.1, 0.25, 0.5]\n",
    "\n",
    "def find_cutoff_at_percentiles(array: np.ndarray):\n",
    "    _percentiles = sorted(list(set(percentiles + [1 - _p for _p in percentiles])))\n",
    "\n",
    "    cutoff_values = np.quantile(array, _percentiles)\n",
    "    print(\"\\n\".join([f\"{_percentile:.5f}\\t{_cutoff:.0f}\" for _percentile, _cutoff in zip(_percentiles, cutoff_values)]))\n",
    "    return cutoff_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the signal region for our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21460,
     "status": "ok",
     "timestamp": 1584617999737,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "EXKW6pb65GCu",
    "outputId": "d59f7969-e156-4525-b840-34c575db8b06"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_b_mass_distribution = Bs_df['_kkmumu_mass']\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "\n",
    "for quantile in find_cutoff_at_percentiles(_b_mass_distribution.loc[(_b_mass_distribution > 2500) & (_b_mass_distribution < 3750)]):\n",
    "    plt.axvline(quantile, color='k', alpha=0.3)\n",
    "\n",
    "n, bins, patches = plt.hist(_b_mass_distribution, range=[0, 8e3], bins='auto', alpha=0.7, density=True)\n",
    "plt.xlim((2e3, 8e3))\n",
    "plt.xlabel(r'$m_{K K \\mu \\mu}$')\n",
    "plt.tight_layout(pad=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21453,
     "status": "ok",
     "timestamp": 1584617999738,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "DaQ9z__f6tRU",
    "outputId": "7e01b122-b332-48ac-9735-23719102ab06"
   },
   "outputs": [],
   "source": [
    "#Bs_signal_region = [3103, 5624]\n",
    "#Bs_signal_region = [5268, 5476]\n",
    "Bs_signal_region = [2500,3750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8K9Hzovj57jT"
   },
   "source": [
    "We have defined the signal region as the region inclusive of the $\\pm 2 \\sigma$ region of a normal distribution.\n",
    "\n",
    "I.e. The mass limits that cover the 2.275th percentile to the 97.725th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ib8D_O_l7iOM"
   },
   "outputs": [],
   "source": [
    "_kkmumu_mass = 5366\n",
    "\n",
    "def get_bs_kkmumu_mass_cut_mask(data_frame):\n",
    "    \"\"\"\n",
    "    Returns a boolean mask that removes the lb -> pkmumu peak.\n",
    "    I.e. False when event would fall inside the pkmumu peak.\n",
    "    \"\"\"\n",
    "    _kkmumu_mass_width = 150\n",
    "\n",
    "    mass_cut_mask = (data_frame._kkmumu_mass > _kkmumu_mass + _kkmumu_mass_width) | (data_frame._kkmumu_mass < _kkmumu_mass - _kkmumu_mass_width)\n",
    "    print(f\"kKmumu mass cut:\\n{mass_cut_mask.value_counts()}\")\n",
    "    return mass_cut_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the background region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFpXcpKk7bZP"
   },
   "outputs": [],
   "source": [
    "def get_and_plot_background_data(data_frame):\n",
    "    bs_kkmumu_mass_cut_mask = get_bs_kkmumu_mass_cut_mask(data_frame)\n",
    "\n",
    "    no_kkmumu_data_frame = data_frame.loc[bs_kkmumu_mass_cut_mask]\n",
    "\n",
    "    background_df = Bs_df.loc[\n",
    "         ((Bs_df['jpsi_M'] > 3020) & (Bs_df['jpsi_M'] < 3155) & (Bs_df['Bs_M'] > 5200) & (Bs_df['jpsi_M'] < 5500)) | \n",
    "        ((Bs_df['Bs_M']>5600) & ((Bs_df['jpsi_M'] < 3020) | (Bs_df['jpsi_M'] > 3155)) & ((Bs_df['jpsi_M'] < 3536) | (Bs_df['jpsi_M'] > 3873)))\n",
    "    ]\n",
    "\n",
    "    \n",
    "    background_counts_left = (background_df._kkmumu_mass < Bs_signal_region[0]).sum()\n",
    "    background_counts_right = (background_df._kkmumu_mass > Bs_signal_region[1]).sum()\n",
    "    background_counts_str = f\"Left: {background_counts_left}, Right: {background_counts_right}\"\n",
    "\n",
    "    signal_counts = len(no_kkmumu_data_frame) - background_counts_left - background_counts_right\n",
    "    print(f\"Counts:\\n\\tSignal: {signal_counts}\\n\\tBackground: {background_counts_str}\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(6, 5), sharex='all', sharey='all')\n",
    "    plot_range = (2000, 7000)\n",
    "    ax1.hist(no_kkmumu_data_frame._kkmumu_mass, bins=100, range=plot_range, label='$B_s$ data')\n",
    "    ax1.axvspan(*Bs_signal_region, color='red', alpha=0.3, label=f'Signal region: {signal_counts}\\n($\\pm 2\\sigma$: 2.275 to 97.725 percentiles)')\n",
    "\n",
    "    ax2.hist(background_df._kkmumu_mass, bins=100, range=plot_range, label=f'Background Proxy: {background_counts_str}')\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax2.set_xlim(plot_range)\n",
    "    ax2.set_xlabel(r'$m_{KK \\mu\\mu}$')\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    return background_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22455,
     "status": "ok",
     "timestamp": 1584618000754,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "cdu-6wBL8p6f",
    "outputId": "c47d68fa-2245-47d6-e9ef-d6b7e4982ffa"
   },
   "outputs": [],
   "source": [
    "Bs_background_df = get_and_plot_background_data(Bs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXkbCphYCkip"
   },
   "source": [
    "## Expected background in signal region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpY_ceqACoLC"
   },
   "outputs": [],
   "source": [
    "def find_expected_background_in_signal(background_data_frame):\n",
    "    \"\"\"\n",
    "    Calculates the expected background in the signal region.\n",
    "    \n",
    "    Calculates the average density of candidates in the L left bins and R right bins.\n",
    "    Extrapolates the average density across the width of the signal region.\n",
    "\n",
    "    Here, L = 0, R = 627.\n",
    "    \"\"\"\n",
    "    bg_mass = background_data_frame._kkmumu_mass\n",
    "\n",
    "    bin_width = 50\n",
    "    left_bins = 0\n",
    "    right_bins = 627\n",
    "\n",
    "    count_left = ((bg_mass > Bs_signal_region[0] - left_bins * bin_width) & (bg_mass < Bs_signal_region[0])).sum()\n",
    "    count_right = ((bg_mass < Bs_signal_region[1] + right_bins * bin_width) & (bg_mass > Bs_signal_region[1])).sum()\n",
    "\n",
    "    print(f\"Left: {count_left} / {bin_width * left_bins} MeV\")\n",
    "    print(f\"Right: {count_right} / {bin_width * right_bins} MeV\")\n",
    "    average_density = (count_left + count_right) / ((left_bins + right_bins) * bin_width)\n",
    "    print(f\"Average density: {average_density:.3f} / MeV\")\n",
    "\n",
    "    signal_region_width = Bs_signal_region[1] - Bs_signal_region[0]\n",
    "    expected_background_in_signal = average_density * signal_region_width\n",
    "    print(f\"Expected background in signal region: {expected_background_in_signal:.2f}\")\n",
    "    return expected_background_in_signal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22446,
     "status": "ok",
     "timestamp": 1584618000756,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "QBOglxoIT6Oi",
    "outputId": "b2c4fe9f-f0d4-4af9-cf45-d0ecb9fb3815"
   },
   "outputs": [],
   "source": [
    "expected_background_in_signal = find_expected_background_in_signal(Bs_background_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hMOWEpwp2OrC"
   },
   "source": [
    "# Machine Learning Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for Boosted Decision Tree (BDT) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UEfL-Up2Z7B"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_x_and_y():\n",
    "\n",
    "    cut_signal = Bs_df_signal\n",
    "    signal = cut_signal.loc[:, BDT_COLUMNS]\n",
    "    background = Bs_background_df.loc[:, BDT_COLUMNS]\n",
    "\n",
    "    x = pd.concat([signal, background], axis=0, ignore_index=True)\n",
    "    y1, y2 = np.ones(len(signal)), np.zeros(len(background))\n",
    "    y = np.concatenate([y1, y2], axis=None)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22437,
     "status": "ok",
     "timestamp": 1584618000756,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "mucpJv5WEny6",
    "outputId": "01753355-611b-4a04-872a-dbb6dfa55fcf"
   },
   "outputs": [],
   "source": [
    "X, y = get_x_and_y()\n",
    "print(f\"X: {X.to_numpy().shape}\")\n",
    "print(f\"y: {y.shape}. ({np.unique(y, return_counts=True)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWCF6QPXDfD3"
   },
   "source": [
    "## Punzi Figure of Merit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abg7KppzE1lg"
   },
   "source": [
    "The figure of merit used to choose the threshold at which to cut on.\n",
    "\n",
    "To maximise the sensitivity of the investigation, the figure of merit \n",
    "\n",
    "$\\frac{\\mathrm{TPR}}{\\frac{a}{2} + \\sqrt{\\mathrm{FPR} \\times B}}$\n",
    "\n",
    "has to be maximised.\n",
    "\n",
    "Here, $\\mathrm{FPR} \\times B$ represents the expected background remaining in the signal region after the cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LNfRDzlzDiOK"
   },
   "outputs": [],
   "source": [
    "a = 0.5\n",
    "B = expected_background_in_signal\n",
    "\n",
    "def calculate_punzi_fom(y_test: np.ndarray, y_pred: np.ndarray, thresholds: np.ndarray):\n",
    "    y_test_bool = y_test.astype(bool)\n",
    "\n",
    "    P = y_test.sum()\n",
    "    N = len(y_test) - P\n",
    "\n",
    "    foms_data = defaultdict(list)\n",
    "    for threshold in thresholds:\n",
    "        y_pred_class = y_pred > threshold\n",
    "        TP = y_pred_class[y_test_bool].sum()\n",
    "        TPR = TP / P\n",
    "        FP = y_pred_class[~y_test_bool].sum()\n",
    "        FPR = FP / N\n",
    "        fom = TPR / (a / 2 + (FPR * B) ** 0.5)\n",
    "        for variable in ['P', 'N', 'TP', 'FP', 'TPR', 'FPR', 'fom']:\n",
    "            foms_data[variable].append(eval(variable))\n",
    "    return foms_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBspSZSsF-DV"
   },
   "source": [
    "## Classifier performance evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qgMdJUP_GQih"
   },
   "source": [
    "Here, we evaluate the performance of different model types.\n",
    "\n",
    "These evaluations result in a TPR and FPR and FoM that is our best estimate for the individual models trained later.\n",
    "\n",
    "(It is highly likely that the individual models trained later report slightly misleading TPRs and FPRs and FoMs due to the small number of test data).\n",
    "\n",
    "We use 2-fold CV (where the data is split into equal parts test and train) in an attempt to maximise the amount of test data so the classifier statistics can be better approximated.\n",
    "\n",
    "Better results may be achieved using higher-fold CV (e.g. 4-fold CV, where the data would be split into 3/4 train and 1/4 test), but the calculated model performance statistics would then be even more unreliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CEGfCZU_Dmsh"
   },
   "source": [
    "\n",
    "### Function definitions\n",
    "\n",
    "Trains and evaluates many models using a specific strategy (e.g. 2-Fold CV).\n",
    "\n",
    "In the plots generated, each grey line represents the performance of a single model that has been trained on one randomly-sampled half of the data and tested on the remaining half.\n",
    "\n",
    "The figure of merit (FoM) is aggregated in two forms:\n",
    "1. Calculated for each model, and the mean of all the individual FoMs is taken (orange)\n",
    "2. The mean TPR and mean FPR across all models is taken, and a single FoM is calculated from this (green)\n",
    "\n",
    "We use the threshold that maximises the second, as we also use the mean TPR and \n",
    "mean FPR in our efficiency/expected background calculations.\n",
    "\n",
    "The first method is also extremely sensitive to the low amount of data, which pushes many FPRs to 0 prematurely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jF3nRexCDzh9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from collections import defaultdict\n",
    "import tqdm as tqdm\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "def get_foms(classifier, n_repeats: int = 100, n_splits: int = 2, weight_classes: bool = True):\n",
    "    X, y = get_x_and_y()\n",
    "\n",
    "    cv = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats)\n",
    "\n",
    "    thresholds = np.concatenate([np.linspace(0, 1, 200), np.linspace(0.95, 1, 100)])\n",
    "    thresholds.sort()\n",
    "\n",
    "    foms_datas = []\n",
    "\n",
    "    for i, (train, test) in enumerate(tqdm.tqdm(cv.split(X, y), total=n_splits * n_repeats, desc='Split')):\n",
    "        x_train = X.loc[train]\n",
    "        y_train = y[train]\n",
    "        y_train_bool = y_train.astype(bool)\n",
    "        if weight_classes:\n",
    "            sample_weight = get_sample_weight(y_train_bool)\n",
    "            # print(f\"{y_train_bool.sum()}, {(~y_train_bool).sum()}\")\n",
    "            classifier.fit(x_train, y_train, sample_weight=sample_weight)\n",
    "        else:\n",
    "            classifier.fit(x_train, y_train)\n",
    "        if i==0:\n",
    "            x_train_1=x_train\n",
    "        else:\n",
    "            x_train_2=x_train\n",
    "\n",
    "        x_test = X.loc[test]\n",
    "        y_test = y[test]\n",
    "        # y_test_bool = y_test.astype(bool)\n",
    "\n",
    "        y_pred_probabilities = classifier.predict_proba(x_test)\n",
    "        y_pred = y_pred_probabilities[:, 1]\n",
    "\n",
    "        # print(f\"Length: Train: {len(y_train)}, {y_train.sum()}; Test: {len(y_test)}, {y_test.sum()}. i: {i}\")\n",
    "        # print(f\"Length: Train: {len(y_train)}, {y_train_bool.sum()} True; \"\n",
    "        #       f\"Test: {len(y_test)}, {y_test_bool.sum()} True. i: {i}\")\n",
    "\n",
    "        foms_data = calculate_punzi_fom(y_test, y_pred, thresholds)\n",
    "        foms_datas.append(foms_data)\n",
    "\n",
    "    # print(foms_array.shape)\n",
    "    # np.savez(npz_file, foms=foms_array, thresholds=thresholds)\n",
    "    return foms_datas, thresholds,x_train_1,x_train_2\n",
    "\n",
    "\n",
    "def plot_foms(foms_datas, thresholds):\n",
    "    fig, (ax1, ax2,ax3) = plt.subplots(3, 1, sharex='all', figsize=(6, 7.5))\n",
    "    runs = len(foms_datas)\n",
    "    for i in range(runs):\n",
    "        foms_data = foms_datas[i]\n",
    "        foms = foms_data['fom']\n",
    "        ax1.plot(thresholds, foms, lw=0.5, color='grey', alpha=0.1)\n",
    "    \n",
    "        ax2.plot(thresholds, foms_data['TPR'], '-', color='grey', alpha=0.1)\n",
    "        ax2.plot(thresholds, foms_data['FPR'], '--', color='grey', alpha=0.1)\n",
    "        \n",
    "        ax3.plot(foms_data['FPR'], foms_data['TPR'], color='grey', alpha=0.1)\n",
    "        \n",
    "\n",
    "    foms_array = np.array([data['fom'] for data in foms_datas])\n",
    "    foms_medians = np.median(foms_array, axis=0)\n",
    "    # ax1.plot(thresholds, foms_medians, 'C0', lw=3, alpha=0.5, label='Median')\n",
    "    ax1.plot(thresholds, np.mean(foms_array, axis=0), 'C1', lw=3, alpha=0.5, label='Mean')\n",
    "\n",
    "    tprs_array = np.array([data['TPR'] for data in foms_datas])\n",
    "    tpr_medians = np.median(tprs_array, axis=0)\n",
    "    tprs_means = np.mean(tprs_array, axis=0)\n",
    "    # ax2.plot(thresholds, tpr_medians, 'C0', lw=3, alpha=0.5, label='TPR Median')\n",
    "    ax2.plot(thresholds, tprs_means, 'C1', lw=3, alpha=0.5, label='TPR Mean')\n",
    "\n",
    "    fprs_array = np.array([data['FPR'] for data in foms_datas])\n",
    "    fprs_medians = np.median(fprs_array, axis=0)\n",
    "    fprs_means = np.mean(fprs_array, axis=0)\n",
    "    # ax2.plot(thresholds, fprs_medians, 'C0--', lw=3, alpha=0.5, label='FPR Median')\n",
    "    ax2.plot(thresholds, fprs_means, 'C1--', lw=3, alpha=0.5, label='FPR Mean')\n",
    "    ax3.plot(fprs_means,tprs_means,'C1--', lw=3, alpha=0.5, label='ROC',color=\"indianred\")\n",
    "    \n",
    "    AUROC=sp.integrate.trapz(tprs_means,fprs_means)\n",
    "    \n",
    "    print(\"AUROC: \",abs(AUROC))\n",
    "\n",
    "    foms_from_means = tprs_means / (a / 2 + (fprs_means * B) ** 0.5)\n",
    "    ax1.plot(\n",
    "        thresholds,\n",
    "        foms_from_means,\n",
    "        'C2', lw=3, alpha=0.5,\n",
    "        label='FoM from stat means'\n",
    "    )\n",
    "\n",
    "    foms_argmax = foms_from_means.argmax()\n",
    "    threshold_max = thresholds[foms_argmax]\n",
    "    ax1.axvline(\n",
    "        threshold_max, \n",
    "        label=f'FoM max: {foms_from_means.max():.3f} at {threshold_max:.3f}', \n",
    "        color='k', \n",
    "        lw=1, \n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax2.axvline(\n",
    "        threshold_max, \n",
    "        color='k', \n",
    "        lw=1, \n",
    "        alpha=0.5\n",
    "    )\n",
    "    print(f'Medians: \\tTPR: {tpr_medians[foms_argmax]:.3f}. FPR: {fprs_medians[foms_argmax]:.3f} (at {threshold_max:.3f})')\n",
    "    print(f'Means:   \\tTPR: {tprs_means[foms_argmax]:.3f}. FPR: {fprs_means[foms_argmax]:.3f} (at {threshold_max:.3f})')\n",
    "\n",
    "    print(f'FoMs from means: {foms_from_means[foms_argmax]:.3f}  (at {threshold_max:.3f})')\n",
    "\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax3.grid()\n",
    "\n",
    "    ax2.set_xlabel(\"Classifier Threshold\")\n",
    "    \n",
    "    ax1.set_ylabel(\"Figure of Merit\")\n",
    "    ax2.set_ylabel(\"Rates\")\n",
    "    \n",
    "    ax3.set_xlabel(\"FPR\")\n",
    "    ax3.set_ylabel(\"TPR\")\n",
    "    \n",
    "    \n",
    "    ax1.legend(loc=2)\n",
    "    ax2.legend(loc=10)\n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 5)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fprs_means,tprs_means,'C1--', lw=3, alpha=0.5, label='ROC',color=\"indianred\")\n",
    "    \n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.ylim(0.75,1)\n",
    "    plt.xlim(0,0.1)\n",
    "    plt.show()\n",
    "\n",
    "    mean_tpr = tprs_means[foms_argmax]\n",
    "    mean_fpr = fprs_means[foms_argmax]\n",
    "    return threshold_max, mean_tpr, mean_fpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJKwnZzVD4Oj"
   },
   "outputs": [],
   "source": [
    "def get_sample_weight(y_bool):\n",
    "    sample_weight = np.zeros_like(y_bool, dtype=np.float32)\n",
    "    normalisation = len(y_bool)\n",
    "\n",
    "    sample_weight[y_bool] =  normalisation / y_bool.sum()\n",
    "    sample_weight[~y_bool] =  normalisation / (~y_bool).sum()\n",
    "    return sample_weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H7144bN3D5et"
   },
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UgNhEV9D66R"
   },
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_q9Q0GPRFxWM"
   },
   "source": [
    "#### XGBoost with Dart Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensures the classes are weighted equally despite imbalanced samples.\n",
    "scale_pos_weight = (y==0).sum() / (y==1).sum() * 0.5\n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "09hQbAfRFy_I"
   },
   "outputs": [],
   "source": [
    "def train_dart_booster():\n",
    "    xgb = xgboost.XGBClassifier(\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        booster='dart'\n",
    "    )\n",
    "    foms_datas, thresholds,x_train_1,x_train_2 = get_foms(xgb, n_repeats=100, weight_classes=False)\n",
    "    threshold, tpr, fpr=plot_foms(foms_datas, thresholds)\n",
    "    return threshold, tpr, fpr,x_train_1,x_train_2\n",
    "\n",
    "threshold, tpr, fpr,x_train_1,x_train_2=train_dart_booster()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ws4PK-jkGPL4"
   },
   "source": [
    "## Classifier training and saving\n",
    "Training and evaluation of actual models to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rh9aB_AMHjeJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "\n",
    "def train_model(classifier, plot=False, save=False, is_xgb: bool = False, info_at_threshold=None, n_splits=2):\n",
    "    X, y = get_x_and_y()\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    thresholds = np.linspace(0, 1, 500)\n",
    "    if info_at_threshold :\n",
    "        if info_at_threshold not in thresholds:\n",
    "            thresholds = np.sort(np.concatenate([thresholds, [info_at_threshold]]))\n",
    "        info_at_threshold_index = (thresholds == info_at_threshold).argmax()\n",
    "\n",
    "  \n",
    "    foms_datas = []\n",
    "    y_test_bools = []\n",
    "    y_test_preds = []\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        x_train = X.loc[train]\n",
    "        y_train = y[train]\n",
    "        y_train_bool = y_train.astype(bool)\n",
    "\n",
    "        if is_xgb:\n",
    "            classifier.fit(x_train, y_train)\n",
    "        else:\n",
    "            sample_weight = get_sample_weight(y_train_bool)\n",
    "            classifier.fit(x_train, y_train, sample_weight=sample_weight)\n",
    "\n",
    "        x_test = X.loc[test]\n",
    "        y_test = y[test]\n",
    "        y_test_bool = y_test.astype(bool)\n",
    "\n",
    "        # print(f\"Length: Train: {len(y_train)}, {y_train.sum()}; Test: {len(y_test)}, {y_test.sum()}. i: {i}\")\n",
    "        prob = classifier.predict_proba(x_test)\n",
    "        y_pred = prob[:, 1]\n",
    "        foms_data = calculate_punzi_fom(y_test, y_pred, thresholds)\n",
    "        foms_datas.append(foms_data)\n",
    "        if info_at_threshold:\n",
    "            print(f\"Info at threshold: {info_at_threshold:.3f}:\")\n",
    "            for k, v in foms_data.items():\n",
    "                print(f\"\\t{k}: {v[info_at_threshold_index]:.3f}\")\n",
    "\n",
    "        y_test_bools.append(y_test_bool)\n",
    "        y_test_preds.append(y_pred)\n",
    "\n",
    "        if save:\n",
    "            file_name = f\"classifier_MC{i}\"\n",
    "            if is_xgb:\n",
    "                classifier.save_model(f\"{file_name}.json\")\n",
    "            else:\n",
    "                with open(f\"{file_name}.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(classifier, f)\n",
    "\n",
    "    if plot:\n",
    "        plot_model_performance(foms_datas, thresholds, y_test_bools, y_test_preds, classifier,Bs_df)\n",
    "\n",
    "def plot_model_performance(foms_datas, thresholds, y_test_bools, y_test_preds, classifier,data_frame):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, sharex='all', figsize=(6, 5))\n",
    "    runs = len(foms_datas)\n",
    "    \n",
    "    model_x = data_frame.loc[:, BDT_COLUMNS].loc[(data_frame[\"Bs_M\"]>2500) & (data_frame[\"Bs_M\"]<3750)]\n",
    "    model_x_2 = data_frame.loc[:, BDT_COLUMNS].loc[(data_frame[\"Bs_M\"]>5500)]\n",
    "    model_x_1 = data_frame.loc[:, BDT_COLUMNS].loc[(data_frame['jpsi_M'] > 3020) & (data_frame['jpsi_M'] < 3155)]\n",
    "    model_x_3= Bs_df_phimumu.loc[:, BDT_COLUMNS]\n",
    "    \n",
    "    model_y_pred =  classifier.predict_proba(model_x)[:, 1]\n",
    "    model_y_pred_1 =  classifier.predict_proba(model_x_1)[:, 1]\n",
    "    model_y_pred_2 =  classifier.predict_proba(model_x_2)[:, 1]\n",
    "    model_y_pred_3 =  classifier.predict_proba(model_x_3)[:, 1]\n",
    "    \n",
    "    for i in range(runs):\n",
    "        classifier_i = i + 1\n",
    "        foms_data = foms_datas[i]\n",
    "        foms = foms_data['fom']\n",
    "        ax1.plot(thresholds, foms, lw=1, color=f'C{i}', alpha=0.5, label=f\"Classifier {classifier_i}\")\n",
    "    \n",
    "        ax2.plot(thresholds, foms_data['TPR'], '-', color=f'C{i}', alpha=0.7, label=f'TPR {classifier_i}')\n",
    "        ax2.plot(thresholds, foms_data['FPR'], '--', color=f'C{i}', alpha=0.7, label=f'FPR {classifier_i}')\n",
    "\n",
    "    tprs_array = np.array([data['TPR'] for data in foms_datas])\n",
    "    tprs_means = np.mean(tprs_array, axis=0)\n",
    "    ax2.plot(thresholds, tprs_means, 'C1', lw=3, alpha=0.5, label='TPR Mean')\n",
    "\n",
    "    fprs_array = np.array([data['FPR'] for data in foms_datas])\n",
    "    fprs_means = np.mean(fprs_array, axis=0)\n",
    "    ax2.plot(thresholds, fprs_means, 'C1--', lw=3, alpha=0.5, label='FPR Mean')\n",
    "\n",
    "    foms_from_means = tprs_means / (a / 2 + (fprs_means * B) ** 0.5)\n",
    "    ax1.plot(\n",
    "        thresholds,\n",
    "        foms_from_means,\n",
    "        'r', lw=3, alpha=0.5,\n",
    "        label='FoM from stat means'\n",
    "    )\n",
    "\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax2.set_xlabel(\"Classifier Threshold\")\n",
    "    ax1.set_ylabel(\"Figure of Merit\")\n",
    "    ax2.set_ylabel(\"Rates\")\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fig, axs = plt.subplots(runs, 1, figsize=(6, 5), sharex='all')\n",
    "    for i in range(runs):\n",
    "        ax = axs[i]\n",
    "        y_test_bool = y_test_bools[i]\n",
    "        y_test_pred = y_test_preds[i]\n",
    "        plot_range = [0, 1]\n",
    "        ax.hist(model_y_pred, bins=20, range=plot_range, label='Data', alpha=1,color=\"green\")\n",
    "        ax.hist(y_test_pred[~y_test_bool], bins=20, range=plot_range, label='Not event', alpha=0.8,color=\"lightcoral\")\n",
    "        ax.hist(y_test_pred[y_test_bool], bins=20, range=plot_range, label='Event', alpha=0.5,color=\"dodgerblue\")\n",
    "        ax.legend(loc=9)\n",
    "        ax.grid()\n",
    "        ax.set_yscale('log')\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # plot BDT score\n",
    "    fig = plt.figure()\n",
    "    y_test_bool = y_test_bools[1]\n",
    "    y_test_pred = y_test_preds[1]    \n",
    "    plot_range = [0, 1]    \n",
    "    plt.hist(model_y_pred_3, bins=20, range=plot_range, label=r'$\\phi \\mu \\mu$ MC', alpha=1,color=\"green\")\n",
    "    plt.hist(y_test_pred[~y_test_bool], bins=20, range=plot_range, label='Background Sample', alpha=0.7,color=\"orange\")\n",
    "    plt.hist(y_test_pred[y_test_bool], bins=20, range=plot_range, label='Signal MC', alpha=0.5,color=\"dodgerblue\")\n",
    "    plt.legend(loc=9)\n",
    "    plt.grid()\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # plot BDT score for signal and backgorund proxies\n",
    "    fig = plt.figure()\n",
    "    y_test_bool = y_test_bools[1]\n",
    "    y_test_pred = y_test_preds[1]    \n",
    "    plot_range = [0, 1]\n",
    "    plt.hist(model_y_pred_1, bins=20, range=plot_range, label=r'$J/\\psi \\ \\phi_3$ Sample', alpha=1,color=\"orange\")\n",
    "    plt.hist(model_y_pred, bins=20, range=plot_range, label=r'Comb. Proxy ($m_{KK \\mu \\mu}>5.6$ GeV)', alpha=0.5, color=\"dodgerblue\")\n",
    "    plt.hist(model_y_pred, bins=40, range=plot_range, label=r'Signal Region ($m_{KK \\mu \\mu}<3.6$ GeV)',color=\"dodgerblue\")\n",
    "    plt.legend(loc=9)\n",
    "    plt.grid()\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.axvspan(0.8, 1, color='black', alpha=1)\n",
    "    plt.ylim(bottom=0.8)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    feature_importance = classifier.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_indices.shape[0]) + .5\n",
    "    plt.barh(pos, feature_importance[sorted_indices])\n",
    "    plt.yticks(pos, X.columns[sorted_indices])\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # plotting BDT distributions for training vs testing to check against overfitting\n",
    "    model_y_11 =  classifier.predict_proba(x_train_1)[:, 1]\n",
    "    model_y_12 =  classifier.predict_proba(x_train_2)[:, 1]\n",
    "\n",
    "    plt.figure()\n",
    "    plot_range = [0, 1]\n",
    "    plt.hist(model_y_11, bins=20, range=plot_range, label='Training Set', alpha=1,color=\"orange\")\n",
    "    plt.hist(model_y_12, bins=20, range=plot_range, label='Testing Set', alpha=0.5,color=\"dodgerblue\")\n",
    "    plt.legend(loc=9)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2823,
     "status": "ok",
     "timestamp": 1584619136107,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "_sOq6KAFHoDK",
    "outputId": "d1619b51-7563-4170-bc84-7a6c2f27d80a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(37621)\n",
    "xgb = xgboost.XGBClassifier(scale_pos_weight=scale_pos_weight)\n",
    "#train_model(classifier=xgb, plot=True, save=False, is_xgb=True, info_at_threshold=threshold)\n",
    "train_model(classifier=xgb, plot=True, save=True, is_xgb=True, info_at_threshold=0.744)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK9o8nQlKu37"
   },
   "source": [
    "# Branching Fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPZt_pRpKzKr"
   },
   "source": [
    "## Applying ML model to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YnT462xiMGUo"
   },
   "outputs": [],
   "source": [
    "def plot_mass_distributions(data_frame, bins='auto'):\n",
    "    bs_kkmumu_mass_cut_mask = get_bs_kkmumu_mass_cut_mask(data_frame)\n",
    "    plt.figure(figsize=[6,3])\n",
    "    plot_range = (2500, 3750)\n",
    "    plt.hist(data_frame._kkmumu_mass, bins=bins, range=plot_range)\n",
    "    plt.axvline(_kkmumu_mass, color='k', alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.xlabel(r'$m_{KK\\mu\\mu}$')\n",
    "    plt.xlim(plot_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aG8EqKXbK18k"
   },
   "outputs": [],
   "source": [
    "def apply_and_plot_ml_model(classifier, threshold, data_frame, expected_FPR):\n",
    "    model_x = data_frame.loc[:, BDT_COLUMNS]\n",
    "    model_y_pred =  classifier.predict_proba(model_x)[:, 1]\n",
    "    model_y_pred_class = model_y_pred > threshold\n",
    "\n",
    "    after_model_data_frame = data_frame.loc[model_y_pred_class]\n",
    "    plot_mass_distributions(after_model_data_frame, 100)\n",
    "    plot_mass_distributions(data_frame, 100)\n",
    "    plt.axvspan(*Bs_signal_region, color='C5', alpha=0.3)\n",
    "\n",
    "    print(f\"Total remaining: {len(after_model_data_frame)}\")\n",
    "\n",
    "    bs_kkmumu_mass_cut_mask = get_bs_kkmumu_mass_cut_mask(after_model_data_frame)\n",
    "    after_model_after_kkmumu_mass_cut = after_model_data_frame.loc[bs_kkmumu_mass_cut_mask]\n",
    "    print(f\"After Bs=KKmumu mass cut: {len(after_model_after_kkmumu_mass_cut)}\")\n",
    "\n",
    "    remaining_in_signal_region = after_model_after_kkmumu_mass_cut.loc[\n",
    "        (after_model_after_kkmumu_mass_cut._kkmumu_mass > Bs_signal_region[0]) \n",
    "        & (after_model_after_kkmumu_mass_cut._kkmumu_mass < Bs_signal_region[1])\n",
    "    ]\n",
    "    remaining_in_signal_region.to_pickle(\"remaining_in_signal_region.pkl.gz\")\n",
    "\n",
    "    print(f\"Total remaining in signal region: {len(remaining_in_signal_region)}\")\n",
    "\n",
    "    print(f\"Expected background in signal region: {B:.3f} (before ML classifier), {B * expected_FPR:.3f} (after ML classifier)\")\n",
    "    return len(remaining_in_signal_region),after_model_data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2090,
     "status": "ok",
     "timestamp": 1584620253484,
     "user": {
      "displayName": "Harry Xie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjzb5jiAQXOWADXBiVG0U4iMfRBvXF8SOtwqK3ScQ=s64",
      "userId": "02352922071773947928"
     },
     "user_tz": 0
    },
    "id": "qSKNSNXAKyi8",
    "outputId": "cd37fb65-847e-4266-a10a-889555579fa0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb = xgboost.XGBClassifier()\n",
    "xgb.load_model('classifier_MC1.json')\n",
    "remaining_in_signal_region,after_model_data_frame = apply_and_plot_ml_model(xgb, threshold=threshold, data_frame=Bs_df, expected_FPR=fpr)\n",
    "plt.savefig('cleaned_data_after_ml_model.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the data before and after the ML classifier is applied for the dimuon and dikaon mass distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.744\n",
    "\n",
    "signal_region_mask=(((Bs_df['phi_M']>1600) & (Bs_df['phi_M']<1820)) & \n",
    "                    ((Bs_df['jpsi_M'] < 3020) | (Bs_df['jpsi_M'] > 3155)) & ((Bs_df['jpsi_M'] < 3536) | (Bs_df['jpsi_M'] > 3873))\n",
    "                   )\n",
    "\n",
    "signal_region_df = Bs_df.loc[signal_region_mask]\n",
    "model_x = signal_region_df.loc[:, BDT_COLUMNS]\n",
    "model_y_pred =  xgb.predict_proba(model_x)[:, 1]\n",
    "model_y_pred_class = model_y_pred > threshold\n",
    "\n",
    "model_x_MC = Bs_df_signal.loc[:, BDT_COLUMNS]\n",
    "model_y_pred_MC =  xgb.predict_proba(model_x_MC)[:, 1]\n",
    "model_y_pred_class_MC = model_y_pred_MC > threshold\n",
    "\n",
    "after_model_data_frame = signal_region_df.loc[model_y_pred_class]\n",
    "after_model_MC=Bs_df_signal.loc[model_y_pred_class_MC]\n",
    "\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "plt.hist(signal_region_df['phi_M'],bins=100,range=[800,2500]) \n",
    "plt.hist(after_model_data_frame['phi_M'],bins=100,range=[800,2500], alpha=0.7)   \n",
    "plt.xlabel(\"$m_{KK}$ data\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "plt.hist(signal_region_df['Bs_M'],bins=100,range=[2000,6000])                    \n",
    "plt.hist(after_model_data_frame['Bs_M'],bins=100,range=[2000,6000], alpha=0.7)\n",
    "plt.xlabel(\"$m_{KK \\mu \\mu}$ data\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "plt.hist(Bs_df_signal['phi_M'],bins=100,range=[800,2500]) \n",
    "plt.hist(after_model_MC['phi_M'],bins=100,range=[800,2500], alpha=0.7)   \n",
    "plt.xlabel(\"$m_{KK}$ MC\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[6,3])\n",
    "plt.hist(Bs_df_signal['Bs_M'],bins=100,range=[2000,6000])                    \n",
    "plt.hist(after_model_MC['Bs_M'],bins=100,range=[2000,6000], alpha=0.7)\n",
    "plt.xlabel(\"$m_{KK \\mu \\mu}$ MC\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Data events pre BDT: \",len(signal_region_df['_kkmumu_mass']))\n",
    "print(\"Data events retained: \",len(after_model_data_frame['_kkmumu_mass']))\n",
    "\n",
    "print(\"MC events pre BDT: \",len(Bs_df_signal['Bs_M']))\n",
    "print(\"MC events retained: \",len(after_model_MC['Bs_M']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of the variables that the BDT used the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpsiphi3mask=(Bs_df['jpsi_M'] > 3020) & (Bs_df['jpsi_M'] < 3155) & (Bs_df['Bs_M'] > 5200) & (Bs_df['jpsi_M'] < 5500)\n",
    "combinatorial=(Bs_df['Bs_M']>5600) & ((Bs_df['jpsi_M'] < 3020) | (Bs_df['jpsi_M'] > 3155)) & ((Bs_df['jpsi_M'] < 3536) | (Bs_df['jpsi_M'] > 3873))\n",
    "\n",
    "model_y_rej_class_MC = model_y_pred_MC < threshold\n",
    "\n",
    "rejected_model_MC=Bs_df_signal.loc[model_y_rej_class_MC]\n",
    "\n",
    "model_x = Bs_df.loc[:, BDT_COLUMNS]\n",
    "model_y_pred =  xgb.predict_proba(model_x)[:, 1]\n",
    "model_y_rej_class = model_y_pred < threshold\n",
    "\n",
    "rejected_model_jpsi=Bs_df.loc[model_y_rej_class].loc[jpsiphi3mask]\n",
    "rejected_model_comb=Bs_df.loc[model_y_rej_class].loc[combinatorial]\n",
    "\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 10)\n",
    "plt.hist(after_model_MC['_tau_minus_flight_dist'], bins=30, range=Bs_mass_range,color=\"orange\",label=r\"BDT score $>$ 0.744\",density=True)\n",
    "plt.hist(rejected_model_MC['_tau_minus_flight_dist'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"BDT score $>$ 0.744\",alpha=0.5,density=True)\n",
    "plt.xlabel(r'$\\tau$ flight distance [mm]')\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 0.004)\n",
    "plt.hist(after_model_MC['_tau_minus_closest_dist'], bins=30, range=Bs_mass_range,color=\"orange\",label=r\"BDT score $>$ 0.744\",density=True)\n",
    "plt.hist(rejected_model_MC['_tau_minus_closest_dist'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"BDT score $<$ 0.744\",alpha=0.5,density=True)\n",
    "plt.xlabel(r'$\\tau$ CDA [mm]')\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 0.5)\n",
    "plt.hist(after_model_MC['_tau_minus_phi_IP'], bins=30, range=Bs_mass_range,color=\"orange\",label=r\"BDT score $>$ 0.744\",density=True)\n",
    "plt.hist(rejected_model_MC['_tau_minus_phi_IP'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"BDT score $<$ 0.744\",alpha=0.5,density=True)\n",
    "plt.xlabel(r'$\\tau$ IP $\\phi$ [mm]')\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "\n",
    "#########\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 10)\n",
    "plt.hist(Bs_df_signal['_tau_minus_flight_dist'], bins=30, range=Bs_mass_range,color=\"orange\",label=\"MC Signal\",density=True)\n",
    "plt.hist(Bs_df.loc[jpsiphi3mask]['_tau_minus_flight_dist'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"$J/\\psi \\phi_3$\",alpha=0.5,density=True)\n",
    "plt.hist(Bs_df.loc[combinatorial]['_tau_minus_flight_dist'], bins=30, range=Bs_mass_range,color=\"green\",label=\"Combinatorial\",alpha=0.3,density=True)\n",
    "plt.xlabel(r'$\\tau$ flight distance [mm]')\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 0.001)\n",
    "plt.hist(Bs_df_signal['_tau_minus_closest_dist'], bins=30, range=Bs_mass_range,color=\"orange\",label=\"MC Signal\",density=True)\n",
    "plt.hist(Bs_df.loc[jpsiphi3mask]['_tau_minus_closest_dist'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"$J/\\psi \\phi_3$\",alpha=0.5,density=True)\n",
    "plt.hist(Bs_df.loc[combinatorial]['_tau_minus_closest_dist'], bins=30, range=Bs_mass_range,color=\"green\",label=\"Combinatorial\",alpha=0.3,density=True)\n",
    "plt.xlabel(r'$\\mu$ CDA [mm]')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "Bs_mass_range = (0, 0.5)\n",
    "plt.hist(Bs_df_signal['_tau_minus_phi_IP'], bins=30, range=Bs_mass_range,color=\"orange\",label=\"MC Signal\",density=True)\n",
    "plt.hist(Bs_df.loc[jpsiphi3mask]['_tau_minus_phi_IP'], bins=30, range=Bs_mass_range,color=\"dodgerblue\",label=r\"$J/\\psi \\phi_3$\",alpha=0.5,density=True)\n",
    "plt.hist(Bs_df.loc[combinatorial]['_tau_minus_phi_IP'], bins=30, range=Bs_mass_range,color=\"green\",label=\"Combinatorial\",alpha=0.3,density=True)\n",
    "plt.xlabel(r'$\\mu$ IP $\\phi_3$ [mm]')\n",
    "plt.xlim(Bs_mass_range)\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rustic visualization of correlation between CDA and IP. Might help us understand why IP is used by BDT very much, but IP distribution of signal and background is not that different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "Bs_mass_range = (0, 0.5)\n",
    "plt.scatter(Bs_df.loc[combinatorial]['_tau_minus_phi_IP'],Bs_df.loc[combinatorial]['_tau_minus_closest_dist'],color=\"green\",label=\"Combinatorial\")  \n",
    "plt.scatter(Bs_df.loc[jpsiphi3mask]['_tau_minus_phi_IP'],Bs_df.loc[jpsiphi3mask]['_tau_minus_closest_dist'],color=\"dodgerblue\",label=r\"$J/\\psi \\phi_3$\")\n",
    "plt.scatter(Bs_df_signal['_tau_minus_phi_IP'],Bs_df_signal['_tau_minus_closest_dist'],color=\"red\", label=\"MC signal\") \n",
    "plt.ylim(-0.0001,0.003)\n",
    "plt.xlim(-0.0001,5)\n",
    "plt.xlabel(r'$\\mu$ IP $\\phi_3$ [mm]')\n",
    "plt.ylabel(r'$\\mu$ CDA [mm]')\n",
    "plt.legend()\n",
    "plt.tight_layout(pad=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Lb to pKmutau: ML and Branching Fraction (Part 2 of 2)",
   "provenance": [
    {
     "file_id": "1wshh3fN5HYtxyFfACwQkO7lEpaioqyYU",
     "timestamp": 1592999666081
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
